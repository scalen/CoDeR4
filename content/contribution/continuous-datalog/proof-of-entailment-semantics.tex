\begin{nestedsection}{Entailment Semantics: Proof By Inductive Lemma}{semantics: proof for entailments}
	\begin{thrm}\labelhyp{continuous datalog: entailment} \hfill \\
		For any ${\Pi^{C,r} = \langle R, F^0, S, r\} \models \langle P, N \rangle}$, where ${\Pi^{C,r}_{i} \models E^0 \cup E^T_{i}}$ for any ${i \in \mathbb{T}}$, the set of facts in a window of any range ${x \geq r}$ over the stream $P$ at a given instant ${i}$ and not in a window of any range ${y \geq x}$ over the stream $N$ at the same instant $i$ is equal to the set of facts $E^T_{i}$ transitively entailed by the ${\Pi^{C,r}}$ at that instant $i$:
		\begin{equation*}
			W^{P,x}_{i} \setminus W^{N,y}_{i} = E_{i}
		\end{equation*}
	\end{thrm}
	In order to prove \refhyp{continuous datalog: entailment}, in which the windows over the entailed streams have a Range greater than or equal to that over the input streams, I present Lemma~\ref{hyp:continuous datalog: entailment lemma} as the general case where the windows have an arbitrary range.
	\begin{lem}\label{hyp:continuous datalog: entailment lemma} \hfill \\
		For any ${\Pi^{C,r} \models \langle P,N \rangle}$, where ${\Pi^{C,r}_{i} \models E^0 \cup E^T_i}$ for any ${i \in \mathbb{T}}$, the set of facts in a window of any range ${x \in \mathbb{N}}$ over the stream $P$ at a given instant ${i}$ and not in a window of any range ${y \geq x}$ over the stream $N$ at the same instant $i$ is equal to the set $E^T_i$ of transient entailments at that instant $i$ less the intersection of all previous sets of transient entailments from instant ${i-1}$ up to and including ${i-X}$:
		\begin{equation*}
			W^{P,x}_{i} \setminus W^{N,y}_{i} = E^T_{i} \setminus \mathop{\cap}^{x}_{d=1} E^T_{i-d}
		\end{equation*}
	\end{lem}

	\begin{proof}[Proof of \refhyp{continuous datalog: entailment} using Lemma~\ref{hyp:continuous datalog: entailment lemma}]\hfill\\
		Assume that Lemma~\ref{hyp:continuous datalog: entailment lemma} holds for every widow size ${x \geq r}$ for some ${\Pi^{C,r} = \langle R, F^0, S, r \rangle}$ and ${y \geq x}$ at every instant ${i \in \mathbb{T}}$.
		It follows that:
		\begin{align*}
			E^T_{i} & = W^{P,x}_{i} \setminus W^{N,y}_{i} \\
			\text{Lemma~\ref{hyp:continuous datalog: entailment lemma}} & = E^T_{i} \setminus \mathop{\cap}^{x}_{d=1} E^T_{i-d} \\
			& = \mathop{\cup}^{x}_{d=1} \left( E^T_{i} \setminus E^T_{i-d} \right) \\
			& = \left( E^T_{i} \setminus E^T_{i-x} \right) \cup \mathop{\cup}^{x-1}_{d=1} \left( E^T_{i} \setminus E^T_{i-d} \right) \\
			\text{Axiom~\ref{axiom:continuous datalog: entailment disjointness}} & = E^T_{i} \cup \mathop{\cup}^{x-1}_{d=1} \left( E^T_{i} \setminus E^T_{i-d} \right) \\
			& = E^T_{i} \cup \left( E^T_{i} \setminus \mathop{\cap}^{x-1}_{d=1} E^T_{i-d} \right) \\
			& = E^T_{i} && \qedhere
		\end{align*}
	\end{proof}

	Lemma~\ref{hyp:continuous datalog: entailment lemma} may be shown to hold for all ${x \in \mathbb{N}}$, regardless of the range of the ${\Pi^{C,r}}$ entailing $P$ and $N$.

	\begin{proof}[Proof of Lemma~\ref{hyp:continuous datalog: entailment lemma} by Induction]
		\textbf{Inductive Hypothesis:} Given that Lemma~\ref{hyp:continuous datalog: entailment lemma} holds for windows of range ${x = k}$ and ${y \geq x}$ over the entailed streams $P$ and $N$, Lemma~\ref{hyp:continuous datalog: entailment lemma} will hold for windows of range ${x = k + 1}$, where ${k \in \mathbb{N}}$.

		Firstly, the \textbf{general case} can be defined in terms of sub-windows of range $1$ by Axiom~\ref{axiom:continuous datalog: window composition} over the entailed streams $P$ and $N$, for which Axioms~\ref{axiom:continuous datalog: positive window increment}~and~\ref{axiom:continuous datalog: negative window increment} provide an equivalence to sets of entailments.
		\begin{align*}
			W^{P,x}_{i} \setminus W^{P,y}_{i} & = \\
			\text{Axiom~\ref{axiom:continuous datalog: window composition}} & = \mathop{\cup}^{x-1}_{d=0}W^{P,1}_{i-d} \setminus \mathop{\cup}^{y-1}_{j=0}W^{N,1}_{i-j} \\
			& = \mathop{\cup}^{x-1}_{d=0} \left( W^{P,1}_{i-d} \setminus \mathop{\cup}^{y-1}_{j=0}W^{N,1}_{i-j} \right) \\
			& = \mathop{\cup}^{x-1}_{d=0}\mathop{\cap}^{y-1}_{j=0} \left( W^{P,1}_{i-d} \setminus W^{N,1}_{i-j} \right) \\
			\text{Axiom~\ref{axiom:continuous datalog: entailment precedes negation}, } y \geq x & = \mathop{\cup}^{x-1}_{d=0}\mathop{\cap}^{i}_{j=0} \left( W^{P,1}_{i-d} \setminus W^{N,1}_{i-j} \right)
		\end{align*}
		The general case also shows that any negative entailments within the window of range $y$ over $N$ not in the window of range $x$ over $N$ do not influence the composition of the set of values ${W^{P,x}_{i} \setminus W^{N,y}_{i}}$.

		Next, Lemma~\ref{hyp:continuous datalog: entailment lemma} may be proven for the \textbf{base case}, where ${x = 1}$ (therefore ${y \geq 1}$).
		\begin{align*}
			E^T_{i} \setminus \mathop{\cap}^{1}_{d=1} E^T_{i-1} & = W^{P,1}_{i} \setminus W^{N,y}_{i} \\
			\text{\textbf{General Case}} & = W^{P,1}_{i} \setminus W^{N,1}_{i} \\
			\text{Axioms~\ref{axiom:continuous datalog: positive window increment}~and~\ref{axiom:continuous datalog: negative window increment}} & = \left( E^T_{i} \setminus E^T_{i-1} \right) \setminus \left( E^T_{i-1} \setminus E^T_{i} \right) \\
			& = E^T_{i} \setminus \left( E^T_{i-1} \setminus \left( E^T_{i-1} \setminus E^T_{i} \right) \right) \\
			& = E^T_{i} \setminus \left( E^T_{i-1} \cap E^T_{i} \right) \\
			& = \left( E^T_{i} \setminus E^T_{i-1} \right) \cup \left( E^T_{i} \setminus E^T_{i} \right) \\
			& = E^T_{i} \setminus E^T_{i-1} \\
			& = E^T_{i} \setminus \mathop{\cap}^{1}_{d=1} E^T_{i-d} && \qedhere
		\end{align*}

		Finally, Lemma~\ref{hyp:continuous datalog: entailment lemma} may be proven for the \textbf{inductive case} as in \reffig{continuous datalog: entailment lemma inductive case}, where ${x = k + 1}$ and Lemma~\ref{hyp:continuous datalog: entailment lemma} holds when ${x = k}$.
		\begin{figure*}[p]
			\centering
			\caption[Proof of Entailment Lemma for Continuous Datalog]{Proof of Inductive Case of the Entailment Lemma for Continuous Datalog.}
			\labelfig{continuous datalog: entailment lemma inductive case}
			\begin{align*}
				E^t_{i+1} \setminus \mathop{\cap}^{k+1}_{d=1} E^T_{i+1-d} & = W^{P,k+1}_{i+1} \setminus W^{P,k+1}_{i+1} \\
				\text{\textbf{General Case}} & = \mathop{\cup}^{k-1}_{d=-1}\mathop{\cap}^{d}_{j=-1} \left( W^{P,1}_{i-d} \setminus W^{N,1}_{i-j} \right) \\
				& = \mathop{\cap}^{-1}_{j=-1} \left( W^{P,1}_{i-j} \setminus W^{N,1}_{i-j} \right) \cup \mathop{\cup}^{k-1}_{d=0}\mathop{\cap}^{d}_{j=-1} \left( W^{P,1}_{i-d} \setminus W^{N,1}_{i-j} \right) \\
				& = \left( W^{P,1}_{i+1} \setminus W^{N,1}_{i+1} \right) \cup \mathop{\cup}^{k-1}_{d=0} \left( W^{P,1}_{i-d} \setminus \mathop{\cup}^{d}_{j=-1} W^{N,1}_{i-j} \right) \\
				& = \left( W^{P,1}_{i+1} \setminus W^{N,1}_{i+1} \right) \cup \mathop{\cup}^{k-1}_{d=0} \left( W^{P,1}_{i-d} \setminus \mathop{\cup}^{d}_{j=0} W^{N,1}_{i-j} \setminus W^{N,1}_{i+1} \right) \\
				& = \left( W^{P,1}_{i+1} \cup \mathop{\cup}^{k-1}_{d=0}\mathop{\cap}^{d}_{j=0} \left( W^{P,1}_{i-d} \setminus W^{N,1}_{i-j} \right) \right) \setminus W^{N,1}_{i+1} \\
				& = \left( W^{P,1}_{i+1} \cup \left( W^{P,k}_{i} \setminus W^{N,k}_{i} \right) \right) \setminus W^{N,1}_{i+1} \\
				\text{\textbf{Inductive Step}, Axioms~\ref{axiom:continuous datalog: positive window increment}~and~\ref{axiom:continuous datalog: negative window increment}:}& = \left( \left( E^T_{i+1} \setminus E^T_{i} \right) \cup \left( E^T_{i} \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \right) \setminus \left( E^T_{i} \setminus E^T_{i+1} \right) \\
				& = \left( E^T_{i+1} \setminus E^T_{i} \setminus \left( E^T_{i} \setminus E^T_{i+1} \right) \right) \cup \left( E^T_{i} \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \setminus \left( E^T_{i} \setminus E^T_{i+1} \right) \right) \\
				& = \left( E^T_{i+1} \setminus E^T_{i} \setminus \left( E^T_{i} \setminus E^T_{i+1} \right) \right) \cup \left( E^T_{i} \setminus \left( E^T_{i} \setminus E^T_{i+1} \right) \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \\
				& = \left( E^T_{i+1} \setminus \left( E^T_{i} \cap E^T_{i+1} \right) \right) \cup \left( \left( E^T_{i} \cap E^T_{i+1} \right) \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \\
				& = \left( E^T_{i+1} \setminus E^T_{i} \right) \cup \left( \left( E^T_{i} \cap E^T_{i+1} \right) \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \\
				& = \left( E^T_{i+1} \setminus E^T_{i} \right) \cup \left( E^T_{i+1} \cap \left( E^T_{i} \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \right) \\
				& = \left( E^T_{i+1} \cap \left( E^T_{i} \right^C \right) \cup \left( E^T_{i+1} \cap \left( E^T_{i} \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \right)\\
				& = E^T_{i+1} \cap \left( \left( E^T_{i} \right)^C \cup \left( E^T_{i} \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \right) \\
				& = E^T_{i+1} \setminus \left( E^T_{i} \setminus \left( E^T_{i} \setminus \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \right) \\
				& = E^T_{i+1} \setminus \left( E^T_{i} \cap \mathop{\cap}^{k}_{d=1} E^T_{i-d} \right) \\
				& = E^T_{i+1} \setminus \mathop{\cap}^{k}_{d=0} E^T_{i-d} \\
				& = E^T_{i+1} \setminus \mathop{\cap}^{k+1}_{d=1} E^T_{i+1-d} && \qedhere
			\end{align*}
		\end{figure*}
	\end{proof}
\end{nestedsection}